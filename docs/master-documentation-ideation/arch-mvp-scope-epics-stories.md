### **Reachy Mini â€“ LangGraph Brain + App + Robot Architecture (Final Draft)**

---

## **1.1 High-Level System Architecture**

```
+---------------------------------------------------------------+
|                     Reachy Brain App (UI)                    |
|   - Real-time graph view                                     |
|   - Node inspector / state viewer                             |
|   - Memory browser                                            |
|   - Behavior overrides & debugging tools                      |
+--------------------------â–²------------------------------------+
                           â”‚ WebSocket API
                           â”‚
+--------------------------â”´------------------------------------+
|                 LangGraph Brain (Core AI System)             |
|  - Perception Nodes                                           |
|  - Cognition Nodes                                            |
|  - Skill Nodes (movement, social, idle, optional wheels)      |
|  - Execution Nodes                                             |
|  - Memory & Emotion Nodes                                     |
|  - BrainState shared model                                     |
+--------------------------â–²------------------------------------+
                           â”‚ gRPC/REST/MQTT commands
                           â”‚ Hardware abstraction layer
+--------------------------â”´------------------------------------+
|                      Robot Hardware Layer                     |
|  - Sensors: camera, mic, IMU, proximity                       |
|  - Actuators: head, arms, LEDs                                |
|  - OPTIONAL: wheels / motor drivers                           |
+---------------------------------------------------------------+
```

---

## **1.2 Subsystem Breakdown**

### **A. LangGraph Brain**

The brain is split into **five node families**:

1. **Perception Nodes**
2. **Cognition Nodes**
3. **Skill Nodes**
4. **Execution Nodes**
5. **Memory & Emotion Nodes**

Each node updates portions of `BrainState`.

---

### **A.1 Perception Nodes**

| Node               | Purpose                                                   |
| ------------------ | --------------------------------------------------------- |
| VisionNode         | Detects people/objects, updates world model               |
| AudioIntentNode    | Speech to intent, triggers conversational/cognitive paths |
| ProximityNode      | Obstacle detection, safety triggers                       |
| PoseEstimationNode | Self pose tracking (only used if wheels enabled)          |

> **Note:** Pose node becomes a no-op if wheels not installed.

---

### **A.2 Cognition Nodes**

| Node                 | Purpose                                |
| -------------------- | -------------------------------------- |
| GoalManagerNode      | Creates, prioritizes and deletes goals |
| PlannerNode          | Turns goals into steps                 |
| BehaviorSelectorNode | Chooses which skill to activate        |

---

### **A.3 Skill Nodes**

| Node                        | Purpose                          | Requires Wheels? |
| --------------------------- | -------------------------------- | ---------------- |
| FollowUserSkillNode         | Move robot to maintain distance  | Yes (optional)   |
| NavigateToLocationSkillNode | Indoor navigation                | Yes (optional)   |
| IdleExploreSkillNode        | Wandering + scanning environment | Yes (optional)   |
| SocialInteractionSkillNode  | Speech + gestures                | No               |
| DanceExpressiveSkillNode    | Gestures, animations             | No               |

If wheels are not present, fallback to:

* Head movements
* Arm animations
* Sound
* LED expressive modes
* â€œVirtual navigationâ€ (pretend movement for testing)

---

### **A.4 Execution Nodes**

| Node                | Purpose                                                   |
| ------------------- | --------------------------------------------------------- |
| SafetyFilterNode    | Ensures safe output; disables movement when wheels absent |
| MotorControllerNode | Sends commands to drive + arms + head                     |
| VoiceOutputNode     | Speak via TTS                                             |

---

### **A.5 Memory & Emotion Nodes**

| Node              | Purpose                                     |
| ----------------- | ------------------------------------------- |
| MemoryWriteNode   | Store experiences                           |
| MemoryRecallNode  | Retrieve memories for conversation/behavior |
| EmotionUpdateNode | Modulates personality                       |

---

## **1.3 Data Flow**

### Without wheels:

* Perception â†’ Cognition â†’ Social skills â†’ Voice/motion outputs
  Movement nodes simply skip execution or simulate.

### With wheels:

* Perception â†’ Cognition â†’ Movement skills â†’ Safety â†’ Hardware Motors
  This is an extended but optional pathway.

---

# âœ… **2. MVP SCOPE (4â€“6 Week Build)**

**Goal:** Deliver a working Reachy Mini with a LangGraph brain, functioning socially and perceptively, without wheels.

> Wheels OPTIONAL and saved for **v2**.

---

## **MVP Feature Set**

### **Required**

âœ” Vision detection (humans + basic object detection)
âœ” Wake word + speech â†’ intent pipeline
âœ” Social interaction skill (speech, gestures)
âœ” Live LangGraph brain + state viewer
âœ” Memory (people + simple episodic logs)
âœ” Emotion system (valence + arousal)
âœ” Idle scanning / expressive behaviors
âœ” Safety system (for arms/head only)

### **Optional (stubbed until wheels exist)**

â—» Follow User
â—» Navigate to Location
â—» Explore Mode
â—» Docking

### **Architecture Delivery**

* Fully documented node graph
* BrainState schema
* Plug-in architecture for adding new nodes
* Debug app (graph view + inspector)

---

## **MVP Timeline (Example)**

### **Week 1: Brain Foundation**

* LangGraph skeleton
* BrainState model
* Core nodes (AudioIntent, Vision, MemoryWrite/Recall, SocialSkill)

### **Week 2: Emotion + Behavior Selection**

* EmotionUpdateNode
* BehaviorSelectorNode
* Idle expressive behaviors

### **Week 3: App Implementation**

* Real-time graph view
* Node inspector
* Live BrainState viewer

### **Week 4: Robot Integration**

* TTS
* Head/arm animations
* Vision â†’ interaction mapping
* Safe execution layer

### **Week 5: Memory + Personality**

* Episodic memory
* User preferences
* Personalized greetings

### **Week 6: Polish + Testing**

* Stress test flows
* Fail-safe handling
* Clean architecture export

---

# âœ… **3. EPICS + USER STORIES (Engineering-Ready)**

---

## **EPIC 1 â€“ LangGraph Brain Core**

### User Stories:

1. As a developer, I can define nodes with clear input/output contracts.
2. As a user, I want the robot to update its world understanding continuously.
3. As a researcher, I want to simulate nodes without hardware.

---

## **EPIC 2 â€“ Perception Layer**

### Stories:

1. VisionNode detects faces and emits metadata.
2. AudioIntentNode converts speech to structured intents.
3. ProximityNode emits safety events.

---

## **EPIC 3 â€“ Social Interaction System**

### Stories:

1. Robot responds conversationally to user intents.
2. Robot gestures while speaking.
3. Emotional state influences tone and motion.

---

## **EPIC 4 â€“ Memory + Personality System**

### Stories:

1. Robot remembers users it has met.
2. Robot recalls past sessions to contextualize responses.
3. Robot maintains an emotional vector updated by events.

---

## **EPIC 5 â€“ Reachy Brain App UI**

### Stories:

1. Developer can see entire graph in real time.
2. Developer can inspect BrainState and override variables.
3. Developer can pause/resume execution steps.
4. Developer can load graph versions and diff outputs.

---

## **EPIC 6 â€“ Movement (Optional Wheels)**

### Stories:

1. With wheels installed, robot follows user.
2. Robot navigates to a designated point.
3. Robot avoids obstacles during navigation.

---

# âœ… **4. VISUAL SYSTEM DIAGRAMS (TEXT-BASED)**

---

## **4.1 System Data Flow Diagram**

```
Sensors â†’ Perception Nodes â†’ Cognition Nodes â†’ Skills â†’ Execution â†’ Hardware
                          â†˜ Memory & Emotion â†—
```

---

## **4.2 Component Diagram**

```
+------------------+        +--------------------+
|    Camera        |------->|    VisionNode      |
+------------------+        +--------------------+
                                    |
+------------------+                v
|   Microphone     |------->| AudioIntentNode    |
+------------------+                |
                                    v
+------------------+       +---------------------+
| Proximity Sensor |----->|  ProximityNode      |
+------------------+       +---------------------+

      Perception feeds into â†“â†“â†“

+------------------------------------------------+
|             Cognition Layer                    |
|  GoalManager â†’ Planner â†’ BehaviorSelector      |
+------------------------------------------------+

                â†“ Skill selection

+------------------------------------------------+
|                Skill Layer                     |
| SocialSkill | IdleExpressive | (NavSkills*)    |
+------------------------------------------------+

                â†“ Actions

+------------------------------------------------+
|             Execution Layer                    |
| SafetyFilter â†’ MotorController â†’ Hardware      |
+------------------------------------------------+

              (* optional wheels)
```

---

# ðŸŽ‰ **ALL FOUR DELIVERABLES COMPLETE**

This is now a **full PRD + architecture + epics + MVP plan + diagrams**, ready for a real engineering team.

---
